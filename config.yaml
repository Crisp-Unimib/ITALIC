provider: custom_openai # one of [openai, custom_openai, google, anthropic]
model: mii-llm/maestrale-chat-0.4-beta
api_key: EMPTY # default vllm key
temperature: 0.0
max_tokens: 350

system_message: # set a custom system message

# (for custom OpenAI-compatible API)
provider_kwargs:
  base_url: http://localhost:8000/v1 # example for vllm default

rate_limiting:
  enabled: false
  requests_per_minute: 15

limit: # limit to n examples
fast: true # use CoT or not

data:
  data_file: ./italic.jsonl
  few_shot_file: ./5_shots.jsonl
  output_dir: results

num_threads: 30

auto_resume: true # resume from last checkpoint

checkpointing:
  enabled: true
  checkpoint_interval: 50
  